{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory for RAG documents\n",
    "# Note: Change this to match your environment\n",
    "documents_dir = \"/mnt/c/Users/your_username/Documents/your_documents_directory/\"\n",
    "documents_pattern = \"*.md\"\n",
    "\n",
    "# Model specification\n",
    "embedding_model_name_or_path = \"sbintuitions/sarashina-embedding-v1-1b\"\n",
    "model_name_or_path = \"sbintuitions/sarashina2.2-3b-instruct-v0.1\"\n",
    "\n",
    "lang = \"ja\"  # or \"en\" for English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from transformers.generation.streamers import TextStreamer\n",
    "from transformers.models.auto.modeling_auto import AutoModelForCausalLM\n",
    "from transformers.models.auto.tokenization_auto import AutoTokenizer\n",
    "from transformers.utils.quantization_config import BitsAndBytesConfig\n",
    "\n",
    "# Load and split documents\n",
    "loader = DirectoryLoader(\n",
    "    documents_dir,\n",
    "    glob=documents_pattern,\n",
    "    loader_cls=TextLoader,\n",
    "    loader_kwargs={\"encoding\": \"utf-8\"},\n",
    ")\n",
    "documents = loader.load()\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "docs = splitter.split_documents(documents)\n",
    "\n",
    "# Prepare embedding model\n",
    "# https://python.langchain.com/api_reference/huggingface/embeddings/langchain_huggingface.embeddings.huggingface.HuggingFaceEmbeddings.html\n",
    "# Note1: Not compatible with quantization by BitsAndBytes\n",
    "# Note2: With only 8GB GPU, running the embedding model on GPU is tight, so running on CPU\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=embedding_model_name_or_path,\n",
    "    model_kwargs={\"device\": \"cpu\"},\n",
    ")\n",
    "\n",
    "# Store in vector DB (FAISS)\n",
    "db = FAISS.from_documents(docs, embedding_model)\n",
    "\n",
    "# Prepare LLM\n",
    "# Note: Without 8bit quantization by BitsAndBytes, it overflows my 8GB GPU :P and becomes extremely slow\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    quantization_config=quant_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _rag(retriever, user_prompt):\n",
    "    \"\"\"Search for similar documents and create a prompt for RAG\"\"\"\n",
    "    relevant_docs = retriever.invoke(user_prompt)\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "\n",
    "    rag_prompt_en = f\"\"\"Answer the question using only the conversation with the user and the information provided below.\n",
    "    ==========================\n",
    "    {context}\n",
    "    ==========================\n",
    "    \"\"\"\n",
    "\n",
    "    rag_prompt_ja = f\"\"\"ユーザーとのやり取りと、次の記載にある情報のみを使って質問に答えてください。\n",
    "    ==========================\n",
    "    {context}\n",
    "    ==========================\n",
    "    \"\"\"\n",
    "\n",
    "    if lang == \"en\":\n",
    "        return rag_prompt_en\n",
    "    elif lang == \"ja\":\n",
    "        return rag_prompt_ja\n",
    "\n",
    "\n",
    "def _build_token(tokenizer, prompts, device):\n",
    "    \"\"\"Build token for input to model\"\"\"\n",
    "\n",
    "    formatted_text = tokenizer.apply_chat_template(\n",
    "        prompts,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        formatted_text,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=1024,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(device)\n",
    "\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def _extract_response(text: str) -> str:\n",
    "    \"\"\"Extract response from AI output.\"\"\"\n",
    "    text = text.split(\"<|assistant|>\")[-1]\n",
    "    text = text.split(\"</s>\")[0]\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def generate(streamer, tokenizer, model, retriever, chat_history, user_prompt) -> str:\n",
    "    \"\"\"\n",
    "    Receive user input and generate AI response using RAG\n",
    "    :param streamer: Object for streaming output\n",
    "    :param tokenizer: Tokenizer\n",
    "    :param model: Model\n",
    "    :param retriever: Retriever for vector DB\n",
    "    :param chat_history: Chat history\n",
    "    :param user_prompt: User input\n",
    "    \"\"\"\n",
    "\n",
    "    rag_prompt = _rag(retriever, user_prompt)\n",
    "\n",
    "    prompts = [{\"role\": \"system\", \"content\": rag_prompt}] + chat_history\n",
    "\n",
    "    inputs = _build_token(tokenizer, prompts, model.device)\n",
    "\n",
    "    # Output from model via streamer, then store the same content in outputs\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        streamer=streamer,\n",
    "        max_new_tokens=512,\n",
    "        use_cache=True,\n",
    "        # Use randomness when choosing words\n",
    "        do_sample=True,  # If False, the model always picks the most likely next word. Default False\n",
    "        temperature=1.0,  # Higher temperature = more randomness. Default 1.0\n",
    "        top_k=50,  # Limits the selection of next words. Default 50\n",
    "        top_p=1.0,  # top_p=1.0 means no limit; top_p=0.9 would restrict to the top words that make up 90% of the probability. Default 1.0\n",
    "    )\n",
    "\n",
    "    output = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    response = _extract_response(output)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For streaming output from the model\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "# Create retriever from the vector DB\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "# Initialize chat history list\n",
    "chat_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(user_prompt):\n",
    "    print(f\"USER: {user_prompt}\\n\")\n",
    "    chat_history.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "\n",
    "    print(\"AI:\")\n",
    "    response = generate(streamer, tokenizer, model, retriever, chat_history, user_prompt)\n",
    "    chat_history.append({\"role\": \"assistant\", \"content\": response})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "chat(\"ゆうちょ銀行で口座開設しようとしたことあったよね？どうなった？\")\n",
    "# or chat(\"What happened when you went to San Francisco?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
